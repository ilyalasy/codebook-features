model_args:
  tokenizer_name: roneneldan/TinyStories-1M

data_args:
  dataset_name: roneneldan/TinyStories
  dataset_config_name: null
  block_size: 2048
  # max_train_samples: 2
  # max_eval_samples: 2
  streaming: False #True

training_args:
  run_name: tiny_model
  output_dir: output_tiny
  overwrite_output_dir: True
  do_train: True
  do_eval: True
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2
  learning_rate: 5e-4  
  warmup_ratio: 0.01
  lr_scheduler_type: polynomial
  # num_train_epochs: 10
  max_steps: 35000
  logging_first_step: True
  logging_steps: 50
  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 2
  load_best_model_at_end: True  
  train_model_params: True
  # model_lr_factor: 0.1
  codebook_reg_p: null
  codebook_weight_decay: 0.01

codebook_args:
  codebook_at: ["mlp"]
  codebook_type: ["vanilla"]
  num_codebooks: [1]
  k_codebook: [2]
  num_codes: 10
  layers_to_snap: all
  similarity_metric: inner_product
  loss: aeloss
  kmeans_init: False
  codebook_kwargs: null
  replace_codes: True

k_scheduler_kwargs: null

model_config_args:
  model_path: roneneldan/TinyStories-1M
  continue_training: False
  # model_type: gptneox
  # hidden_size: 128
  # intermediate_size: 512
  # num_hidden_layers: 4
  # num_attention_heads: 4
  # rotary_emb_base: 10000
  # seq_len: 128
  # vocab_size: 11



apply_codebook: False
enable_logging: True
wandb_charts: False
pretrained_path: null
get_baseline: False
tag_keys: [loss,learning_rate,lr_scheduler_type, warmup_ratio]
tags: ["tiny_main_nocodebook",]
project: codebook